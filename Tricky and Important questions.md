1) Which component of Retrieval-Augmented Generation (RAG) evaluates and prioritizes the information retrieved by the retrieval system?
	**Answer:**  Ranker
2) What advantages does fine-tuning offer in terms of improving model efficiency?
	**Answer:** It reduces the number of tokens needed for model performance
3) What is the key effect of deleting a data source used by and agent in Generative AI Agents?
	**Answer:**: The agent no longer answer questions related to the deleted source
4) What is on `OnDemandServiceMode` class do
	**Answer**: It specifies that the Generative AI model should serve requests only on demand, rather than continuously.
5) Which statement is true about RAG?
	**Answer**: It is non parametric and can theoretically answer questions about any corpus
6) Why diffusion model can't generate text properly?
	**Answer**: Because text representation is categorical, unlike images
7) What are port range that must be specified in the subnet's ingress rule for and Oracle database in OCI generative AI Agents
	**Answer**: 1521-1522
8) How many numerical value are generated for each input phases in cohere.embeded-english-light-v3.0
	**Answer**:384
9) In which scenario is soft prompting more appropriate compared to other training style?
	**Answer**: When there is need to add learnable parameters to a LLM without task specific traning
10) Which fine tuning are supported in cohere.command-r-08-2024 model in OCI Generative AI
	**Answer**: T-Few and LoRA
11) What happens to the model behaviour when you don't provide a value for the seed parameters
	**Answer**: It gives diverse responses
12) LCEL -> Allow for component replacement in declarative manner in building LLM Application
13) How many endpoint can single dedicated ai cluster have
	**Answer**: 30
14) What happens to chat data and retrived context after the session ends:
	**Answer:** Deleted
15) When does a chain typically interact with memory in a run within the LangChain framework?
	**Answer:** After user input but before chain execution, and again after core logic but before output
16) What do prompt templates use for templating in language model applications?
	**Answer:** Python's `str.format` syntax
17) Which statement describes the difference between "Top k" and "Top p" in selecting the next token in the OCI Generative AI Generation models?
	**Answer:****.**  "Top k" selects the next token based on its position in the list of probable tokens, whereas "Top p" selects based on the cumulative probability of the top tokens.
18) Given the following code block:  
	history = StreamlitChatMessageHistory(key="chat_messages")  
	memory = ConversationBufferMemory(chat_memory=history)  
	Which statement is NOT true about StreamlitChatMessageHistory?
	**Answer:** StreamlitChatMessageHistory can be used in any type of LLM application.
19) Which is NOT a built-in memory type in LangChain?
	**Answer**: ConversationImageMemory
20) Which statement is true about string prompt templates and their capability regarding variables?
	**Answer**: They support any number of variables, including the possibility of having none.
21) Which is NOT a category of pretrained foundational models available in the OCI Generative AI service?
	**Answer:** Translation model
22) Total unit hours = 2 unit * hours